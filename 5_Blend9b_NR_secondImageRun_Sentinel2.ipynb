{"cells":[{"cell_type":"markdown","metadata":{"id":"qsxlBnpOELXn"},"source":["# Imports and Authorizations"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":46898,"status":"ok","timestamp":1683592759566,"user":{"displayName":"McKenzie Woodman","userId":"15909959633117312617"},"user_tz":240},"id":"3qSnkbcaCrVX","outputId":"ed1764ad-0a5a-4a8d-fda3-93fa58447c6d"},"outputs":[{"output_type":"stream","name":"stdout","text":["To authorize access needed by Earth Engine, open the following URL in a web browser and follow the instructions. If the web browser does not start automatically, please manually browse the URL below.\n","\n","    https://code.earthengine.google.com/client-auth?scopes=https%3A//www.googleapis.com/auth/earthengine%20https%3A//www.googleapis.com/auth/devstorage.full_control&request_id=nmcpAVXAguft9ShDzkDvKUu9pW7Q26YKY7QQ03tE8jU&tc=cInW51HqzNLknrcXDNBt4YaZcgappXFu-HbTGPv0uoo&cc=vXImGzZxgst2OH78rt751WQpnk6YOKrtMpopxVD-j0A\n","\n","The authorization workflow will generate a code, which you should paste in the box below.\n","Enter verification code: 4/1AbUR2VNiO9Ku3x2YST5yE-FlAxOzVX-RnqDfDTvpVcpyPhUw1RyrxlWWzaU\n","\n","Successfully saved authorization token.\n","Mounted at /content/gdrive\n"]}],"source":["# Imports used in this notebook.\n","import ee\n","import os\n","import sys\n","import time\n","import shutil\n","import pandas as pd\n","import seaborn as sea\n","import numpy as np\n","import matplotlib as mpl\n","import matplotlib.pyplot as plt\n","from ee import batch\n","from matplotlib import cm\n","from google.colab import drive\n","\n","# Must authenticate your EE account before use of the package.\n","ee.Authenticate()\n","ee.Initialize()\n","\n","# This is how we can access our drive to get the correlation product.\n","drive.mount('/content/gdrive')"]},{"cell_type":"markdown","metadata":{"id":"H30mKkUJrm8k"},"source":["# Set the Biscayne Bay AOI."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QN8d_PMwrqf0"},"outputs":[],"source":["#Florida DEP bounds\n","#Indian River Lagoon\n","siteName = 'Indian River Lagoon'\n","siteName_2char = 'IRL'\n","upperLeft = [-80.763739, 28.621909]\n","lowerRight = [-80.503601,  27.605387]\n","# upperLeft = [-80.763739, 28.621909]\n","# lowerRight = [-80.533771,   28.063011]\n","# Define a region of interest to filter our collection.\n","roi = ee.Geometry.Rectangle(upperLeft[0], upperLeft[1], lowerRight[0], lowerRight[1])"]},{"cell_type":"markdown","metadata":{"id":"PgxyZEC2cAbU"},"source":["# Create a list of dates to use for image extraction"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KhQrWCNu0If2"},"outputs":[],"source":["# List of dates.\n","date_list = [\n","# ee.Date('2020-01-01'),\n","# ee.Date('2020-01-06'),\n","# ee.Date('2020-01-16'),\n","# ee.Date('2020-02-03'),\n","# ee.Date('2020-02-08'),\n","# ee.Date('2020-03-01'),\n","# ee.Date('2020-03-14'),\n","# ee.Date('2020-03-24'),\n","# ee.Date('2020-03-29'),\n","# ee.Date('2020-04-03'),\n","# ee.Date('2020-04-08'),\n","# ee.Date('2020-05-05'),\n","# ee.Date('2020-05-20'),\n","# ee.Date('2020-06-24'),\n","# ee.Date('2020-06-29'),\n","# ee.Date('2020-07-14'),\n","# ee.Date('2020-07-19'),\n","# ee.Date('2020-07-24'),\n","# ee.Date('2020-08-08'),\n","# ee.Date('2020-08-13'),\n","# ee.Date('2020-09-02'),\n","# ee.Date('2020-09-17'),\n","# ee.Date('2020-10-15'),\n","# ee.Date('2020-10-27'),\n","# ee.Date('2020-11-24'),\n","# ee.Date('2020-12-09'),\n","# ee.Date('2020-12-21'),\n","# ee.Date('2020-12-29'),\n","# ee.Date('2021-01-05'),\n","# ee.Date('2021-01-20'),\n","# ee.Date('2021-01-28'),\n","# ee.Date('2021-02-02'),\n","# ee.Date('2021-02-19'),\n","# ee.Date('2021-02-22'),\n","# ee.Date('2021-02-24'),\n","# ee.Date('2021-02-27'),\n","# ee.Date('2021-03-01'),\n","# ee.Date('2021-03-04'),\n","# ee.Date('2021-03-11'),\n","# ee.Date('2021-03-14'),\n","# ee.Date('2021-03-19'),\n","# ee.Date('2021-03-24'),\n","# ee.Date('2021-04-05'),\n","# ee.Date('2021-04-10'),\n","# ee.Date('2021-04-30'),\n","# ee.Date('2021-05-05'),\n","# ee.Date('2021-05-10'),\n","# ee.Date('2021-05-15'),\n","# ee.Date('2021-05-25'),\n","# ee.Date('2021-06-09'),\n","ee.Date('2021-07-19'),\n","# ee.Date('2021-07-29'),\n","# ee.Date('2021-09-07'),\n","# ee.Date('2021-09-22'),\n","# ee.Date('2021-09-27'),\n","# ee.Date('2021-09-30'),\n","# ee.Date('2021-10-02'),\n","# ee.Date('2021-10-12'),\n","# ee.Date('2021-11-16'),\n","# ee.Date('2021-11-26'),\n","# ee.Date('2021-11-29'),\n","# ee.Date('2021-12-04'),\n","# ee.Date('2021-12-06'),\n","# ee.Date('2021-12-09'),\n","# ee.Date('2021-12-16'),\n","# ee.Date('2021-12-24'),\n","# ee.Date('2021-12-26'),\n","# ee.Date('2021-12-29'),\n","# ee.Date('2022-01-03'),\n","# ee.Date('2022-01-08'),\n","# ee.Date('2022-01-15'),\n","# ee.Date('2022-01-18'),\n","# ee.Date('2022-01-20'),\n","# ee.Date('2022-01-30'),\n","# ee.Date('2022-02-04'),\n","# ee.Date('2022-02-12'),\n","# ee.Date('2022-02-14'),\n","# ee.Date('2022-02-22'),\n","# ee.Date('2022-02-24'),\n","# ee.Date('2022-03-04'),\n","# ee.Date('2022-03-21'),\n","# ee.Date('2022-03-26'),\n","# ee.Date('2022-04-05'),\n","# ee.Date('2022-04-10'),\n","# ee.Date('2022-04-25'),\n","# ee.Date('2022-05-05'),\n","# ee.Date('2022-05-15'),\n","# ee.Date('2022-06-24'),\n","# ee.Date('2022-07-19'),\n","# ee.Date('2022-07-29'),\n","# ee.Date('2022-08-01'),\n","# ee.Date('2022-08-03'),\n","# ee.Date('2022-08-18'),\n","# ee.Date('2022-09-07'),\n","# ee.Date('2022-09-12'),\n","# ee.Date('2022-09-22'),\n","# ee.Date('2022-10-02'),\n","# ee.Date('2022-10-05'),\n","# ee.Date('2022-10-07'),\n","]"]},{"cell_type":"markdown","metadata":{"id":"gcJ1NhdSrtG5"},"source":["# Select our Sentinel-2 imagery\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0-IjlgsyAsgX"},"outputs":[],"source":["im_list = []\n","\n","# Start Standard code for processing images from a collection.\n","for date in date_list:\n","\n","  # Bring in our imagery.\n","  coll = (ee.ImageCollection(\"COPERNICUS/S2_SR_HARMONIZED\")\n","              .filterBounds(roi)\n","              .filterDate(date, date.advance(1,'day')))\n","\n","  # Clip our input image to our region of interest.\n","  # Set the name of the original image to our new image.\n","  water = coll.mosaic().clip(roi).set({'name':coll.first().get('system:index')})\n","\n","  # Add the newly created image to the empty list created above.\n","  im_list += [water]"]},{"cell_type":"markdown","metadata":{"id":"rZsfGEMGjW-4"},"source":["# Begin analysis"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GIryi9D2ttQ9"},"outputs":[],"source":["# Name the parent folder where we're writing our output files.\n","# output_folder = '/content/gdrive/Shared drives/Asgard/FL_DEP_Seagrass_KSU_ST/Sentinel2/part2'\n","output_folder = '/content/gdrive/My Drive/test_FL_DEP/part2'\n","\n","# Define the Sentinel-2 bands to extract\n","non_ee_band_list = ['B1','B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8', 'B8A']\n","# For reductions.\n","native_res = 10\n","maxP = 1e10\n","\n","# Array of wavelength corrections for Sentinel-2 sensor.\n","wl = ee.Array([443.9,496.6,560.0,664.5,703.9,740.2,782.5,835.1,864.8])\n","# wl = ee.Array([442.7,492.7,559.8,664.4,704.1,740.5,782.8,832.8,864.7])\n","\n","# Name the band combo for outputs.\n","bandCombo=\"B1-B8A\"\n","\n","\"\"\" Bands and Wavelength Array to select. You can add or subtract any you'd like here and the code\n","    below will account for the change throughout the workflow. \"\"\"\n","\n","# Use this for the derivative calculation.\n","band_list_early_length = len(non_ee_band_list)\n","# We use this everywhere else after the derivative calc (original band list x2).\n","#band_list_length = band_list_early_length * 2\n","band_list_length = band_list_early_length\n","# Use this in a couple for loops throughout the code.\n","band_list_forLoops = range(band_list_length)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"il0CXpMYPl3i"},"outputs":[],"source":["# Use this for the commented sections in the code block below.\n","avg_bandVals = '/content/gdrive/My Drive/test_FL_DEP/s2_avgWeightedBandValues.csv'\n","\n","# Here are our weighted, average band values.\n","bv_pd = pd.read_csv(avg_bandVals, delimiter = ',').drop('Unnamed: 0', axis = 1)['0'].tolist()\n","\n","weighted_band_image = ee.ImageCollection([ee.Image(value) for value in range(band_list_early_length)]).toBands()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9iuEV8AsFnr7"},"outputs":[],"source":["def derivative_images(water):\n","\n","  \"\"\"This section is devoted to modifying the image, making it suitable for\n","     the Derivative Spectra function, and running the DS function.\"\"\"\n","\n","  # Parse out the water pixels in Sentinel 2 A/B MSI image.\n","  # This function masks the cloudy pixels and identifies the water pixels.\n","  def get_water(image):\n","\n","    # Water with cloud check, but note this is the not bit addition method.\n","    water_pixels = (image.select('QA60').eq(0).And(image.select('SCL').eq(6)))\n","    # attempts at masking better\n","    # water_pixels=image.select('QA60')\n","    # cloudBitMask=1 << 10\n","    # cirrusBitMask=1 << 11\n","    # mask = water_pixels.bitwiseAnd(cloudBitMask).eq(0).And(water_pixels.bitwiseAnd(cirrusBitMask).eq(0))\n","    # Mask all non-water pixels in the image.\n","    image = image.updateMask(water_pixels)\n","\n","    return image.select(non_ee_band_list).toDouble()\n","\n","  # Run the S2 mask on the test images.\n","  water_mask = get_water(water)\n","\n","  # Using this for the subtraction of average values, addition of weighted average values.\n","\n","  # Making an image out of the average values. This creates a constant image.\n","  band_averages = water_mask.reduceRegion(ee.Reducer.mean(),roi,maxPixels = maxP, scale = native_res,bestEffort=True).values().getInfo()\n","\n","  single_image_averages = ee.ImageCollection([ee.Image(value) for value in band_averages]).toBands()\n","\n","  # Update the image by first subtracting individual image average, then by adding on the weighted average band values.\n","  water_mask = water_mask.subtract(single_image_averages).add(weighted_band_image)\n","\n","  # Binary reducer, which returns 1 everywhere that water hasn't been masked.\n","  mask_image = water_mask.reduce(ee.Reducer.anyNonZero())\n","\n","  # Any pixels passing the pixel mask will now be polygonized.\n","  water_geom = mask_image.reduceToVectors(scale = native_res, geometry = roi, maxPixels = maxP, bestEffort = True)\n","\n","  # This function will select the bands of importance and clip the raster.\n","  def Clip(img, geo):\n","    return img.clip(geo)\n","\n","  clipped_image = Clip(water_mask,water_geom)\n","\n","  # Add code below to divide the reflectance bands by 10^4 to scale from 0 to 1\n","  clipped_image = clipped_image.multiply(0.0001)\n","\n","  def derivativeSpectra(img,numBands):\n","\n","    drdlraster = None\n","\n","    for i in range(numBands):\n","\n","      # Special case of the equation is required for the first band.\n","      if i == 0:\n","        drdlraster = img.expression(\n","          '(secondBand - firstBand)/(secondIndex - firstIndex)',\n","          {\n","          'secondBand': img.select(i+1),\n","          'firstBand': img.select(i),\n","          'secondIndex': wl.get([i+1]),\n","          'firstIndex': wl.get([i])\n","          }).rename('image_Index' + str(i) + '_derivative')\n","\n","      # Special case of the equation is required for the last band.\n","      elif i == (numBands-1):\n","\n","        drdlraster = img.expression(\n","          '(secondBand - firstBand)/(secondIndex - firstIndex)',\n","          {\n","          'secondBand': img.select(i),\n","          'firstBand': img.select(i-1),\n","          'secondIndex': wl.get([i]),\n","          'firstIndex': wl.get([i-1])\n","          }).rename('image_Index' + str(i) + '_derivative')\n","\n","      # Apply equation for all other bands in the image.\n","      else:\n","        drdlraster = img.expression(\n","          '(secondBand - firstBand)/(secondIndex - firstIndex)',\n","          {\n","          'secondBand': img.select(i+1),\n","          'firstBand': img.select(i-1),\n","          'secondIndex': wl.get([i+1]),\n","          'firstIndex': wl.get([i-1])\n","          }).rename('image_Index' + str(i) + '_derivative')\n","\n","      # Adds derivative bands to the input image.\n","      img = img.addBands(drdlraster)\n","\n","    # Final image has the name of its original image reassigned.\n","    # This is used for file naming conventions later on.\n","    return img.set({'name': water.get('name')}).select(8,9,10,11,12,13,14,15,16)\n","\n","  # Run the Derivative Spectra function on the test image.\n","  d_image = derivativeSpectra(clipped_image,band_list_early_length)\n","\n","  return d_image\n","\n","  # ======================================================== #"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iasoMhwNsFAL"},"outputs":[],"source":["# Make derivative images for all of our images of interest.\n","der_list = [derivative_images(ee.Image(im)) for im in im_list]"]},{"cell_type":"markdown","metadata":{"id":"xelgt_3ReA0R"},"source":["# Priming for VPCA"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7o4nEbVfgPi6"},"outputs":[],"source":["# This code will be used for making some subfolders and writing image data to said folders.\n","\n","# Length of our derivative list.\n","image_list_length = len(der_list)\n","\n","# Using the variables created above, get the names of the images that we are using in our analysis.\n","image_names = [ee.Image(der_list[i]).get('name').getInfo() for i in range(image_list_length)]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ffwjinOS_m1v","outputId":"f6a8b869-eaa1-44d7-ebe4-9ca9a703fc12","executionInfo":{"status":"ok","timestamp":1683592824998,"user_tz":240,"elapsed":3,"user":{"displayName":"McKenzie Woodman","userId":"15909959633117312617"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['20210719T160519_20210719T160513_T17RNL']"]},"metadata":{},"execution_count":17}],"source":["image_names"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DRWsWLCpC2zt","executionInfo":{"status":"error","timestamp":1683592826139,"user_tz":240,"elapsed":146,"user":{"displayName":"McKenzie Woodman","userId":"15909959633117312617"}},"colab":{"base_uri":"https://localhost:8080/","height":183},"outputId":"3a5d370f-c493-42d2-c88e-e78e49b21440"},"outputs":[{"output_type":"error","ename":"FileExistsError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileExistsError\u001b[0m                           Traceback (most recent call last)","\u001b[0;32m<ipython-input-18-1982fdc0daa3>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Make the part2 folder.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mFileExistsError\u001b[0m: [Errno 17] File exists: '/content/gdrive/My Drive/test_FL_DEP/part2'"]}],"source":["# Make the part2 folder.\n","os.mkdir(output_folder)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vat-m1xmEgOD"},"outputs":[],"source":["# This is where each respective image's information will be saved.\n","# Notice that we're using the image names we just made.\n","for name in image_names:\n","  os.mkdir(f'{output_folder}/{name}_outputs')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ei1xqQVXfYmY"},"outputs":[],"source":["# List our directory of folders, which we can utilize to write files.\n","output_folders = os.listdir(output_folder)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SwHirp7_FvQM","executionInfo":{"status":"ok","timestamp":1683592832736,"user_tz":240,"elapsed":257,"user":{"displayName":"McKenzie Woodman","userId":"15909959633117312617"}},"outputId":"59d15086-c8a3-4aba-9590-0f636d025ef3"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['20200101T160511_20200101T160506_T17RNL_outputs',\n"," '20220801T155911_20220801T161100_T17RNL_outputs',\n"," '20220803T160519_20220803T160516_T17RNL_outputs',\n"," '20200106T160509_20200106T160506_T17RNL_outputs',\n"," '20200116T160509_20200116T160505_T17RNL_outputs',\n"," '20200314T160021_20200314T160651_T17RNL_outputs',\n"," '20210224T160511_20210224T160511_T17RNL_outputs',\n"," '20210719T160519_20210719T160513_T17RNL_outputs']"]},"metadata":{},"execution_count":21}],"source":["output_folders"]},{"cell_type":"markdown","metadata":{"id":"ouiB-W0FBv55"},"source":["# Large functions used in the analysis that I've removed for the purpose of shortening the analysis block"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bfDaeSG-rO_w"},"outputs":[],"source":["\"\"\"Section devoted to performing the Varimax Rotation.\"\"\"\n","\n","# This function is used to restructure the array after each pass through the rotation loop.\n","def vpca_toArray(initArr, a, b, xRot, yRot):\n","# Values of \"i\" and \"j\" are checked to return a new, reformatted version of the input array.\n","  if a == 0 and b == 1:\n","    initArr = ee.Array.cat([xRot,yRot,initArr.slice(1,2,3)],1)\n","  elif a == 0 and b == 2:\n","    initArr = ee.Array.cat([xRot,initArr.slice(1,1,2),yRot],1)\n","  elif a == 1 and b == 2:\n","    initArr = ee.Array.cat([initArr.slice(1,0,1),xRot,yRot],1)\n","  else:\n","    initArr = initArr\n","  return initArr\n","\n","def varimax_rotation(input, flag = False):\n","\n","  epsilon = ee.Number(2 * 0.00001)\n","\n","  # Sqrt of communality.\n","  # (Communality: the proportion of variance that each item has in common with other items.)\n","  h = ((input.pow(2)).reduce(reducer=ee.Reducer.sum(),axes=([1]))).sqrt()\n","  # Extend \"h\" to match the dimensions of the input array.\n","  h_b = h.repeat(1,3)\n","  # Input is normalized by the sqrt of communality.\n","  bh = input.divide(h_b)\n","\n","  \"\"\"=======================================================\"\"\"\n","  \"\"\"Create list to be used for iteration. If you need to change\n","    the amount of iterations through the rotate() function,\n","    this is where you do it.\"\"\"\n","  # Max seq. on which this step has been tested and worked: (0,6)\n","  seq = ee.List.sequence(0,6)\n","  \"\"\"=======================================================\"\"\"\n","  def rotate(current,prev):\n","\n","    bh = ee.Array(prev)\n","\n","    for i in range(2):\n","      for j in range(3):\n","        #  Select i'th column from the matrix.\n","        xx = bh.slice(1,i,i+1)\n","        #  Select the j'th column from the matrix.\n","        yy = bh.slice(1,j,j+1)\n","\n","        #  Calculations below are made to determine phi.\n","        uu = (xx.pow(2)).subtract(yy.pow(2))\n","\n","        vv = xx.multiply(yy).multiply(2)\n","\n","        aa = uu.reduce(reducer=ee.Reducer.sum(),axes=([0])).get([0,0])\n","\n","        bb = vv.reduce(reducer=ee.Reducer.sum(),axes=([0])).get([0,0])\n","\n","        cc = (uu.pow(2)).subtract(vv.pow(2)).reduce(\n","                reducer=ee.Reducer.sum(),\n","                axes = ([0])\n","                ).get([0,0])\n","\n","        dd = (uu.multiply(vv).multiply(2)).reduce(\n","                reducer = ee.Reducer.sum(),\n","                axes = ([0])\n","                ).get([0,0])\n","\n","      num = dd.subtract((aa.multiply(bb).multiply(2)).divide(band_list_length))\n","\n","      den = cc.subtract(((aa.pow(2)).subtract(bb.pow(2))).divide(band_list_length))\n","\n","      phi = (den.atan2(num)).divide((band_list_length) - 1)\n","\n","      # Determine if phi is greater than or equal to epsilon.\n","      evaluation = (phi.abs()).gte(epsilon)\n","\n","      # Rotation takes place in server-side if statement.\n","      rot = ee.Algorithms.If(\n","                  condition=ee.Number(evaluation).eq(1),\n","                  trueCase=\n","                  {\n","                    'xx_rot': (xx.multiply(phi.cos())).add(yy.multiply((phi.sin()))),\n","                    'yy_rot': (xx.multiply((phi.multiply(-1)).sin())).add(yy.multiply(phi.cos()))\n","                  },\n","                  falseCase=None)\n","      # Dictionary contains the two rotations.\n","      rot = ee.Dictionary(rot)\n","\n","      # Set the new value of \"xx\".\n","      setX = ee.Algorithms.If(\n","              condition=ee.Number(evaluation).eq(1),\n","              trueCase=ee.Dictionary(rot).getArray('xx_rot'),\n","              falseCase=xx)\n","      # Set the new value of \"yy\".\n","      setY = ee.Algorithms.If(\n","              condition=ee.Number(evaluation).eq(1),\n","              trueCase=ee.Dictionary(rot).getArray('yy_rot'),\n","              falseCase=yy)\n","\n","      # Cast \"xx\" and \"yy\" into Array objects for the reformation function.\n","      xx = ee.Array(setX)\n","      yy = ee.Array(setY)\n","\n","      # Reform the rotated array.\n","      bh = vpca_toArray(bh,i,j,xx,yy)\n","\n","    # Checks the remaining number of pairs, determine if array has converged.\n","    return bh\n","\n","  # Iterate through each position of the list defined above.\n","  seq = seq.iterate(rotate,bh)\n","\n","  # Cast the result of the rotation as new bh.\n","  bh = ee.Array(seq)\n","\n","  # Denormalize bh by multiplying it by the sqrt of communality.\n","  result = bh.multiply(h_b)\n","\n","  fracVar = ((result.pow(2)).reduce(ee.Reducer.sum(),([0]))).divide(band_list_length)\n","\n","  return ee.Dictionary({'result':result,'h':h,'fracVar':fracVar})\n","\n","# Project original data onto component axis to create component scores.\n","def create_scores(data):\n","\n","  # Calculate factor score coefficients.\n","  trans = data.transpose()\n","  v = trans.matrixMultiply(data)\n","  inverse = v.matrixInverse()\n","  b = data.matrixMultiply(inverse)\n","\n","  return b\n","\n","\"\"\"EE table export requires a Feature Collection.\n","   This step reformats the VPCA Loadings into a\n","   Feature, which can be accepted as an input in\n","   a Feature Collection.\"\"\"\n","\n","# Compile stats into EE Feature (necessary for export).\n","def createFeats_loads(sliver):\n","  row1 = sliver.get([0,0])\n","  row2 = sliver.get([0,1])\n","  row3 = sliver.get([0,2])\n","  row4 = sliver.get([0,3])\n","  row5 = sliver.get([0,4])\n","  row6 = sliver.get([0,5])\n","  row7 = sliver.get([0,6])\n","  row8 = sliver.get([0,7])\n","  row9 = sliver.get([0,8])\n","  row10 = sliver.get([0,9])\n","\n","  rowsForCsv = {\n","      'Wavelengths': row1,\n","      'component 1': row2,\n","      'component 2': row3,\n","      'component 3': row4,\n","      'component 4': row5,\n","      'component 5': row6,\n","      'component 6': row7,\n","      'Comm_1-3': row8,\n","      'Comm_4-6': row9,\n","      'Comm_tot': row10\n","      }\n","\n","  return ee.Feature(None,rowsForCsv)\n","\n","def createFeats_eigens(sliver, title = None):\n","  row1 = sliver.get(0)\n","  row2 = sliver.get(1)\n","  row3 = sliver.get(2)\n","  row4 = sliver.get(3)\n","  row5 = sliver.get(4)\n","  row6 = sliver.get(5)\n","\n","  rowsForCsv = {\n","      'Wavelengths': title,\n","      'component 1': row1,\n","      'component 2': row2,\n","      'component 3': row3,\n","      'component 4': row4,\n","      'component 5': row5,\n","      'component 6': row6,\n","      'Comm_1-3': 'Total',\n","      'Comm_4-6': sliver.slice(0,6).reduce(ee.Reducer.sum()),\n","      'Comm_tot': None\n","      }\n","\n","  return ee.Feature(None, rowsForCsv)\n","\n","def createFeats_fracVar(sliver, title = None):\n","  row1 = sliver.get([0])\n","  row2 = sliver.get([1])\n","  row3 = sliver.get([2])\n","  row4 = sliver.get([3])\n","  row5 = sliver.get([4])\n","  row6 = sliver.get([5])\n","\n","  rowsForCsv = {\n","      'Wavelengths': title,\n","      'component 1': row1,\n","      'component 2': row2,\n","      'component 3': row3,\n","      'component 4': row4,\n","      'component 5': row5,\n","      'component 6': row6,\n","      'Comm_1-3': 'Total',\n","      'Comm_4-6': sliver.reduce(ee.Reducer.sum(), axes = [0]).get([0]),\n","      'Comm_tot': None\n","      }\n","\n","  return ee.Feature(None, rowsForCsv)"]},{"cell_type":"markdown","metadata":{"id":"lJb6_8PstAM9"},"source":["# Full analysis function"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PSrjRyNc0P-4","executionInfo":{"status":"ok","timestamp":1683592842118,"user_tz":240,"elapsed":2,"user":{"displayName":"McKenzie Woodman","userId":"15909959633117312617"}},"outputId":"8fa476fa-e5f4-47d7-f85e-a2121642cdb8"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[<ee.image.Image at 0x7fafcefbf670>]"]},"metadata":{},"execution_count":23}],"source":["der_list"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-7eMNKGSbJbS"},"outputs":[],"source":["def fullAnalysis(fn, image, out_name, out_fold):\n","  # First, load our csv file into a Pandas DataFrame.\n","  drive_string = fn\n","  dataframe = pd.read_csv(drive_string, delimiter = ',').drop('Unnamed: 0',1)\n","\n","  # Next, convert our dataframe to a numpy array.\n","  corr_ls = dataframe.to_numpy()\n","\n","  # Eigenvalues and eigenvectors calculated using Numpy.\n","  eigens = np.linalg.eigh(corr_ls)\n","\n","  # Extract the eigenvalues from eigens so that we can reverse the sort\n","  eigenVals = eigens[0]\n","\n","  # Reverse the eigenvalue sort so that they are in descending order\n","  eigenVals = eigenVals[::-1]\n","  # Convert to an EE list.\n","  eigenVals = ee.List(list(eigenVals))\n","\n","  # Build the eigenvector matrix\n","  # Rotate the eigenvector matrix 180 to place it in the correct orientiation to match\n","  # the eigenvalues sorted in descending order\n","  rotating = np.rot90(eigens[1],2)\n","\n","  # Take each row of our eigenvector matrix and concatenate the rows in a new EE array.\n","  eigenVecs = ee.Array.cat([ee.Array(list(rotating[i])) for i in range(len(rotating))])\n","\n","  # Reformat the array. May need to be multiplied by -1.\n","  eigenVecs = eigenVecs.reshape([band_list_length,band_list_length]).transpose()\n","\n","  # Checking results...\n","  # for row in eigenVecs.getInfo():\n","  #   print(row)\n","\n","  # ================= #\n","  \"\"\"The following section is devoted to pre-processing\n","   the data for the Varimax Rotation.\"\"\"\n","\n","  # Determine whether or not each eigenvalue is positive.\n","  def checkEigs(eig):\n","      # Cast to an ee Number.\n","      eig = ee.Number(eig)\n","      check = ee.Algorithms.If(eig.lt(0),eig.multiply(-1),eig.multiply(1))\n","      return check\n","\n","  # All eigenvalues should now be positive.\n","  eigenVals = eigenVals.map(checkEigs)\n","\n","  # Determine square root of all eigenvalues to obtain the singular values.\n","  singvalue = ee.Array(eigenVals).sqrt()\n","\n","  def formatData(rows,value):\n","    new_list = ee.List([])\n","\n","    # Slice array and multiply by each row's respective positive eigenvalue.\n","    for i in band_list_forLoops:\n","      sliver = ee.Algorithms.If(ee.Number(i).eq(band_list_length - 1), rows.slice(0,i).multiply(value.get([i])),\n","                                                    rows.slice(0,i,i+1).multiply(value.get([i])))\n","      # Push formatted rows into new empty list.\n","      new_list = new_list.add(sliver)\n","    return new_list\n","\n","  # Get our formatted information.\n","  pcaload = formatData(eigenVecs,singvalue)\n","\n","  # Select the top three principal components for the rotation.\n","  pc1 = pcaload.getArray(0).project([1]).toList()\n","  pc2 = pcaload.getArray(1).project([1]).toList()\n","  pc3 = pcaload.getArray(2).project([1]).toList()\n","  # Second batch of PCs.\n","  pc4 = pcaload.getArray(3).project([1]).toList()\n","  pc5 = pcaload.getArray(4).project([1]).toList()\n","  pc6 = pcaload.getArray(5).project([1]).toList()\n","\n","\n","  # Create the input arrays for rotation.\n","  varimaxInput_1 = ee.Array([pc1,pc2,pc3]).transpose()\n","  varimaxInput_2 = ee.Array([pc4,pc5,pc6]).transpose()\n","\n","  # THIS IS WHERE I USED TO HAVE THE VPCA FUNCTION.\n","\n","  # Run the varimax on PCs 1-3, then PCs 4-6.\n","  vpca_set1 = varimax_rotation(varimaxInput_1)\n","  vpca_set2 = varimax_rotation(varimaxInput_2)\n","\n","  # Retrieves the resultant array from the dictionary output.\n","  batch1 = ee.Array(vpca_set1.get('result'))\n","  batch2 = ee.Array(vpca_set2.get('result'))\n","\n","  # Project original data onto component axis to create component scores.\n","  batch1_scores = create_scores(batch1)\n","  batch2_scores = create_scores(batch2)\n","  all_scores = ee.Array.cat([batch1_scores,batch2_scores],1)\n","\n","  # Cast \"b\" into an Image object. This will be used for the matrix multiplication.\n","  b_img = ee.Image(all_scores)\n","  # Convert the derivative Image to an Array Image.\n","  derivative_array = ee.Image(image).toArray(0).toArray(1)\n","  # Transpose the Array Image.\n","  derivative_array = derivative_array.arrayTranspose()\n","  # Dot product of Array Image and \"b\" Image.\n","  applied_b = derivative_array.matrixMultiply(b_img)\n","  # Transpose the array. This is an optional step to project the bands onto a column.\n","  applied_b = applied_b.arrayTranspose()\n","  # newImage = ee.Image(applied_b)\n","\n","  # Get \"h\" for the stats output, raise to 2nd power.\n","  communality_1 = ee.Array(vpca_set1.get('h')).pow(2)\n","  communality_2 = ee.Array(vpca_set2.get('h')).pow(2)\n","\n","  # Get fractions of variance.\n","  frac_var_set1 = ee.Array(vpca_set1.get('fracVar'))\n","  frac_var_set2 = ee.Array(vpca_set2.get('fracVar'))\n","  joined_frac_var = ee.Array.cat([frac_var_set1,frac_var_set2],1)\n","\n","  # ===== #\n","  \"\"\"This code block sorts the principal components using the fraction\n","     of variance array.\"\"\"\n","\n","  # Combine the two sets of vpca results.\n","  combo = ee.Array.cat([batch1,batch2],1)\n","  # Append the fractions of variance to the bottom of the stack.\n","  useToSort = ee.Array.cat([combo, joined_frac_var],0)\n","\n","  # Use the fractions of variance to sort the vpca results in ascending order.\n","  bottomVals = useToSort.slice(0,band_list_length)\n","  sorted = useToSort.sort(bottomVals)\n","\n","  # Flip our components to descending order.\n","  finalComponents = (ee.Array.cat(\n","    arrays = [\n","              sorted.slice(1,5),\n","              sorted.slice(1,4,5),\n","              sorted.slice(1,3,4),\n","              sorted.slice(1,2,3),\n","              sorted.slice(1,1,2),\n","              sorted.slice(1,0,1)\n","              ],\n","    axis = 1))\n","  # ===== #\n","\n","  # Project fractions of variance down to a one-dimensional array.\n","  frac_var_flat = finalComponents.slice(0,band_list_length).project([1])\n","\n","  # Step 1: compute the sum of the frac_var array.\n","  frac_var_flat_total = frac_var_flat.reduce(ee.Reducer.sum(),[0])\n","\n","  # Step 2: extract the single member of the \"frac_var_flat_total\" array.\n","  frac_var_flat_total = ee.Number(frac_var_flat_total.get([0]))\n","\n","  # Step 3: divide original frac_var_flat by our total observed variance.\n","  ex_frac_var_flat = frac_var_flat.divide(frac_var_flat_total)\n","\n","  # Step 4: compute the sum of the extracted frac_var array. Should be close to 1.0.\n","  ex_frac_var_flat_tot = ex_frac_var_flat.reduce(ee.Reducer.sum(),[0])\n","\n","  # Get PC bands, save fraction of full image variance score to each. This is needed to sort the bands.\n","  one = ee.Image(applied_b.arrayProject([0]).arrayGet(0)).set('frac_score',frac_var_flat.get([0]))\n","  two = ee.Image(applied_b.arrayProject([0]).arrayGet(1)).set('frac_score',frac_var_flat.get([1]))\n","  three = ee.Image(applied_b.arrayProject([0]).arrayGet(2)).set('frac_score',frac_var_flat.get([2]))\n","  four = ee.Image(applied_b.arrayProject([0]).arrayGet(3)).set('frac_score',frac_var_flat.get([3]))\n","  five = ee.Image(applied_b.arrayProject([0]).arrayGet(4)).set('frac_score',frac_var_flat.get([4]))\n","  six = ee.Image(applied_b.arrayProject([0]).arrayGet(5)).set('frac_score',frac_var_flat.get([5]))\n","\n","  # Push PC bands into image collection, sort by frac_score in descending order, flatten into a single image and rename the bands.\n","  finalImage = ee.ImageCollection([one,two,three,four,five,six]).sort('frac_score', False).toBands().rename(['PC_1','PC_2','PC_3','PC_4','PC_5','PC_6'])\n","\n","  # Step 5: save the extracted fractions of variance, along with their total, to the image itself.\n","  finalImage = finalImage.set({\n","    'extracted_fractions_of_variance': ex_frac_var_flat,\n","    'total_variance': ex_frac_var_flat_tot.get([0])\n","  })\n","\n","  # Final Component Loadings file compiled.\n","  communality_tot = communality_1.add(communality_2)\n","\n","  finalLoadings = ee.Array.cat([\n","              wl.reshape([band_list_length,1]),\n","              finalComponents.slice(0,0,band_list_length),\n","              communality_1.reshape([band_list_length,1]),\n","              communality_2.reshape([band_list_length,1]),\n","              communality_tot.reshape([band_list_length,1])],1)\n","\n","  # More efficient way to double the array.\n","  # wl_dub = ee.Array.cat([wl,wl], axis = 0).reshape([band_list_length,1])\n","\n","  # communality_tot = communality_1.add(communality_2)\n","\n","  # Bring together our final array for export.\n","  #finalLoadings = ee.Array.cat([\n","  #              wl_dub,\n","  #              finalComponents.slice(0,0,band_list_length),\n","  #              communality_1.reshape([band_list_length,1]),\n","  #              communality_2.reshape([band_list_length,1]),\n","  #              communality_tot.reshape([band_list_length,1])],1)\n","  # Convert array to a Feature Collection; this is a necessity for Earth Engine's export functionality.\n","  finLoad_export = ee.FeatureCollection([createFeats_loads(finalLoadings.slice(0, i, i+1)) for i in (band_list_forLoops)])\n","\n","  # These are our non-loading stats to export.\n","  supplementary_export = ee.FeatureCollection([\n","      createFeats_eigens(eigenVals, 'Eigenvalues'),\n","      createFeats_fracVar(frac_var_flat, 'Frac Var'),\n","      createFeats_fracVar(ex_frac_var_flat, 'Extracted Frac Var')])\n","\n","  # Merge our two feature collections.\n","  finalFeatColl = finLoad_export.merge(supplementary_export)\n","\n","  # Prepare the export of the VPCA loadings.\n","  componentsExport = batch.Export.table.toDrive(\n","    collection = finalFeatColl,\n","    description = f'6VPCA_{out_name}_finalStats_{bandCombo}',\n","    selectors = ['Wavelengths',\n","                 'component 1',\n","                 'component 2',\n","                 'component 3',\n","                 'component 4',\n","                 'component 5',\n","                 'component 6',\n","                 'Comm_1-3',\n","                 'Comm_4-6',\n","                 'Comm_tot'])\n","\n","  # Initialize the export.\n","  batch.Task.start(componentsExport)\n","\n","  # Monitor the task.\n","  i = 1\n","  while componentsExport.status()['state'] in ['READY', 'RUNNING']:\n","    sys.stdout.write(\"\\r\" + f'Final stats export status update #{i}: ' + str(componentsExport.status()))\n","    sys.stdout.flush()\n","    i += 1\n","    time.sleep(5)\n","\n","  else:\n","    print('Final stats export completed...')\n","    print(componentsExport.status())\n","    time.sleep(10)\n","\n","  # We're moving our file to the desired destination file.\n","  # Output name.\n","  out = f'{output_folder}/{out_fold}'\n","  print(out)\n","  try:\n","    # If the file already exists, get rid of original and replace with the new file.\n","    if os.path.exists(f\"{out}/6VPCA_{out_name}_finalStats_{bandCombo}.csv\"):\n","      os.remove(f\"{out}/6VPCA_{out_name}_finalStats_{bandCombo}.csv\")\n","      # Found here: https://stackoverflow.com/questions/51109931/moving-files-in-google-colab\n","      shutil.move(f'/content/gdrive/My Drive/6VPCA_{out_name}_finalStats_{bandCombo}.csv', out)\n","    else:\n","      shutil.move(f'/content/gdrive/My Drive/6VPCA_{out_name}_finalStats_{bandCombo}.csv', out)\n","\n","  # Let it sleep for a second to let Google Drive catch up.\n","  except:\n","    time.sleep(10)\n","\n","    if os.path.exists(f\"{out}/6VPCA_{out_name}_finalStats_{bandCombo}.csv\"):\n","      os.remove(f\"{out}/6VPCA_{out_name}_finalStats_{bandCombo}.csv\")\n","      shutil.move(f'/content/gdrive/My Drive/6VPCA_{out_name}_finalStats_{bandCombo}.csv', out)\n","    else:\n","      shutil.move(f'/content/gdrive/My Drive/6VPCA_{out_name}_finalStats_{bandCombo}.csv', out)\n","\n","  return [finalImage, ex_frac_var_flat]"]},{"cell_type":"markdown","metadata":{"id":"W7rDLbAtgxcT"},"source":["# Run analysis over all images in stack"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n7dXKfTgcqMe","colab":{"base_uri":"https://localhost:8080/"},"outputId":"93658393-5029-49d4-cd56-ee8a738a7df8","executionInfo":{"status":"ok","timestamp":1683593031543,"user_tz":240,"elapsed":45884,"user":{"displayName":"McKenzie Woodman","userId":"15909959633117312617"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-24-19c2b595e5de>:4: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n","  dataframe = pd.read_csv(drive_string, delimiter = ',').drop('Unnamed: 0',1)\n"]},{"output_type":"stream","name":"stdout","text":["Final stats export status update #2: {'state': 'READY', 'description': '6VPCA_20210719T160519_20210719T160513_T17RNL_finalStats_B1-B8A', 'creation_timestamp_ms': 1683592987490, 'update_timestamp_ms': 1683592987490, 'start_timestamp_ms': 0, 'task_type': 'EXPORT_FEATURES', 'id': 'YRH6YBCIDEVZRFL5J5NLRQNL', 'name': 'projects/earthengine-legacy/operations/YRH6YBCIDEVZRFL5J5NLRQNL'}Final stats export completed...\n","{'state': 'COMPLETED', 'description': '6VPCA_20210719T160519_20210719T160513_T17RNL_finalStats_B1-B8A', 'creation_timestamp_ms': 1683592987490, 'update_timestamp_ms': 1683592996922, 'start_timestamp_ms': 1683592996178, 'task_type': 'EXPORT_FEATURES', 'destination_uris': ['https://drive.google.com/'], 'attempt': 1, 'batch_eecu_usage_seconds': 0.0014739319449290633, 'id': 'YRH6YBCIDEVZRFL5J5NLRQNL', 'name': 'projects/earthengine-legacy/operations/YRH6YBCIDEVZRFL5J5NLRQNL'}\n","/content/gdrive/My Drive/test_FL_DEP/part2/20200101T160511_20200101T160506_T17RNL_outputs\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-24-19c2b595e5de>:4: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n","  dataframe = pd.read_csv(drive_string, delimiter = ',').drop('Unnamed: 0',1)\n"]},{"output_type":"stream","name":"stdout","text":["Final stats export status update #2: {'state': 'READY', 'description': '6VPCA_20210719T160519_20210719T160513_T17RNL_finalStats_B1-B8A', 'creation_timestamp_ms': 1683593010146, 'update_timestamp_ms': 1683593010146, 'start_timestamp_ms': 0, 'task_type': 'EXPORT_FEATURES', 'id': 'SQ5AF44NTGUR7QRR5J6JJ5DY', 'name': 'projects/earthengine-legacy/operations/SQ5AF44NTGUR7QRR5J6JJ5DY'}Final stats export completed...\n","{'state': 'COMPLETED', 'description': '6VPCA_20210719T160519_20210719T160513_T17RNL_finalStats_B1-B8A', 'creation_timestamp_ms': 1683593010146, 'update_timestamp_ms': 1683593018877, 'start_timestamp_ms': 1683593017529, 'task_type': 'EXPORT_FEATURES', 'destination_uris': ['https://drive.google.com/'], 'attempt': 1, 'batch_eecu_usage_seconds': 0.0007409839890897274, 'id': 'SQ5AF44NTGUR7QRR5J6JJ5DY', 'name': 'projects/earthengine-legacy/operations/SQ5AF44NTGUR7QRR5J6JJ5DY'}\n","/content/gdrive/My Drive/test_FL_DEP/part2/20200101T160511_20200101T160506_T17RNL_outputs\n","Image 1 done...\n"]}],"source":["# Empty list to shove our output images into.\n","images = []\n","\n","weighted_matrix = f'/content/gdrive/My Drive/test_FL_DEP/S2_avgCorrelationMatrix.csv'\n","\n","# csv_files, der_list, image_names,output_folders\n","for i in range(len(der_list)):\n","  forSumImages = fullAnalysis(weighted_matrix, der_list[i], image_names[i], output_folders[i])\n","  images += [\n","  fullAnalysis(weighted_matrix, der_list[i], image_names[i], output_folders[i])[0]\n","  ]\n","  print(f'Image {i+1} done...')\n",""]},{"cell_type":"markdown","metadata":{"id":"KJuJrj4v93dF"},"source":["# Image Exports"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GRmJsOGXVU0m","executionInfo":{"status":"ok","timestamp":1683593039482,"user_tz":240,"elapsed":190,"user":{"displayName":"McKenzie Woodman","userId":"15909959633117312617"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"bc8a397d-52ae-4176-bc0b-da58c1a516c0"},"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-27-00238459f61e>:6: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap(obj)`` instead.\n","  ramp = cm.get_cmap(palette_name)\n"]}],"source":["# JOE: I am saving colormaps to variables here.\n","# https://stackoverflow.com/questions/33596491/extract-matplotlib-colormap-in-hex-format\n","\n","def hex_colormap(palette_name = None):\n","  # \"palette_name\" must be a string.\n","  ramp = cm.get_cmap(palette_name)\n","  # Converts color values to hexadecimal format so we can use them.\n","  ramp_ls = [mpl.colors.rgb2hex(ramp(color)[:3]) for color in range(ramp.N)]\n","\n","  return ramp_ls\n","# See a full list of colormaps here: https://matplotlib.org/stable/tutorials/colors/colormaps.html\n","#perceptually uniform colormps\n","viridis = hex_colormap('viridis')\n","inferno = hex_colormap('inferno')\n","magma = hex_colormap('magma')\n","gist_gray = hex_colormap('gist_gray')\n","\n","#divergent (Add _r to invert the color ramp)\n","BrBG = hex_colormap('BrBG')\n","spectral = hex_colormap('Spectral')\n","coolwarm = hex_colormap('coolwarm')\n","seismic = hex_colormap('seismic')\n","\n","BrBG_r = hex_colormap('BrBG_r')\n","spectral_r = hex_colormap('Spectral_r')\n","coolwarm_r = hex_colormap('coolwarm_r')\n","seismic_r = hex_colormap('seismic_r')\n","\n","# Function to find the tails of the pdf\n","# This returns the tails of the pdf for the image with values from p0-p10, and p90-p100\n","# This allows the user to select a range of min-max values as plotting limits\n","\n","def minAndMax(image,region = None):\n","\n","  # Find the percentile values using a reduceRegion call.\n","  min_reducer = image.reduceRegion(\n","      reducer = ee.Reducer.percentile([0,1,2,3,4,5,6,7,8,9,10,90,91,92,93,94,95,96,97,98,99,100],['p0','p1','p2','p3','p4','p5','p6','p7','p8','p9','p10','p90','p91','p92','p93','p94','p95','p96','p97','p98','p99','p100']),\n","          scale = 10,\n","          geometry = region,\n","          bestEffort = True)\n","\n","  orig_keys = min_reducer.keys()\n","\n","  min_reducer = min_reducer.rename([orig_keys.get(0),orig_keys.get(1),orig_keys.get(2),orig_keys.get(3),orig_keys.get(4),orig_keys.get(5),orig_keys.get(6),orig_keys.get(7),orig_keys.get(8),orig_keys.get(9),orig_keys.get(10),orig_keys.get(11),orig_keys.get(12),orig_keys.get(13),orig_keys.get(14),orig_keys.get(15),orig_keys.get(16),orig_keys.get(17),orig_keys.get(18),orig_keys.get(19),orig_keys.get(20),orig_keys.get(21)],['p0','p1','p2','p3','p4','p5','p6','p7','p8','p9','p10','p90','p91','p92','p93','p94','p95','p96','p97','p98','p99','p100'])\n","\n","  return min_reducer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I9P4UZoq3N-4"},"outputs":[],"source":[" #pc1_vals = ee.List([])\n"," #pc2_vals = ee.List([])\n"," #pc3_vals = ee.List([])\n"," #pc4_vals = ee.List([])\n"," #pc5_vals = ee.List([])\n"," #pc6_vals = ee.List([])\n","\n"," #for elem in range(len(images)):\n","   # Convert element to ee.Image.\n"," #  image = ee.Image(images[elem])\n","\n","   # Get min and max values for each principal component, then select out their 2% and 98% stretches.\n","  # minMax_pc1 = minAndMax(image.select('PC_1'), roi)\n","  # pc1_min = minMax_pc1.get('p2')\n","  # pc1_max = minMax_pc1.get('p98')\n","  # pc1_vals = pc1_vals.cat([pc1_min, pc1_max])\n","\n","  # minMax_pc2 = minAndMax(image.select('PC_2'), roi)\n","  # pc2_min = minMax_pc2.get('p2')\n","  # pc2_max = minMax_pc2.get('p98')\n","  # pc2_vals = pc2_vals.cat([pc2_min, pc2_max])\n","\n","  # minMax_pc3 = minAndMax(image.select('PC_3'), roi)\n","  # pc3_min = minMax_pc3.get('p2')\n","  # pc3_max = minMax_pc3.get('p98')\n","  # pc3_vals = pc3_vals.cat([pc3_min, pc3_max])\n","\n","  # minMax_pc4 = minAndMax(image.select('PC_4'), roi)\n","  # pc4_min = minMax_pc4.get('p2')\n","  # pc4_max = minMax_pc4.get('p98')\n","  # pc4_vals = pc4_vals.cat([pc4_min, pc4_max])\n","\n","  # minMax_pc5 = minAndMax(image.select('PC_5'), roi)\n","  # pc5_min = minMax_pc5.get('p2')\n","  # pc5_max = minMax_pc5.get('p98')\n","  # pc5_vals = pc5_vals.cat([pc5_min, pc5_max])\n","\n","  # minMax_pc6 = minAndMax(image.select('PC_6'), roi)\n","  # pc6_min = minMax_pc6.get('p2')\n","  # pc6_max = minMax_pc6.get('p98')\n","  # pc6_vals = pc6_vals.cat([pc6_min, pc6_max])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1UtVyb9-7Ncl"},"outputs":[],"source":["# Conversion to numpy values. This needs to be done because the EE min/max values\n","# need to be reterieved in order to call them as values in the next block.\n"," # pc1_vals_np = np.array(pc1_vals.getInfo())\n"," # pc1_min = np.percentile(pc1_vals_np,2)\n"," # pc1_max = np.percentile(pc1_vals_np,98)\n","\n"," # pc2_vals_np = np.array(pc2_vals.getInfo())\n"," # pc2_min = np.percentile(pc2_vals_np,2)\n"," # pc2_max = np.percentile(pc2_vals_np,98)\n","\n"," # pc3_vals_np = np.array(pc3_vals.getInfo())\n"," # pc3_min = np.percentile(pc3_vals_np,2)\n"," # pc3_max = np.percentile(pc3_vals_np,98)\n","\n"," # pc4_vals_np = np.array(pc4_vals.getInfo())\n"," # pc4_min = np.percentile(pc4_vals_np,2)\n"," # pc4_max = np.percentile(pc4_vals_np,98)\n","\n"," # pc5_vals_np = np.array(pc5_vals.getInfo())\n"," # pc5_min = np.percentile(pc5_vals_np,2)\n"," # pc5_max = np.percentile(pc5_vals_np,98)\n","\n"," # pc6_vals_np = np.array(pc6_vals.getInfo())\n"," # pc6_min = np.percentile(pc6_vals_np,2)\n"," # pc6_max = np.percentile(pc6_vals_np,98)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pTdRZEMMVHwo"},"outputs":[],"source":["# print(pc6_max)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qfOTU84RgSgv"},"outputs":[],"source":["def export_image(image, out_name, out_fold):\n","  # Create visualizations of our output image bands. This reduces the file size considerably.\n","  exportViz_pc1 = image.select('PC_1')\n","  exportViz_pc2 = image.select('PC_2')\n","  exportViz_pc3 = image.select('PC_3')\n","  exportViz_pc4 = image.select('PC_4')\n","  exportViz_pc5 = image.select('PC_5')\n","  exportViz_pc6 = image.select('PC_6')\n","\n","  # Put the bands into a list, so we can loop over the list to start the exports.\n","  exp_ls = [exportViz_pc1,\n","            exportViz_pc2,\n","            exportViz_pc3,\n","            exportViz_pc4,\n","            exportViz_pc5,\n","            exportViz_pc6]\n","\n","  j = 1\n","  for im in exp_ls:\n","    export = batch.Export.image.toDrive(\n","      image = im,\n","      region = roi,\n","      folder = 'test_FL_DEP',\n","      description = f'6VPCA_{out_name}_PC{j}',\n","      scale = native_res,\n","      crs = 'EPSG:4326',\n","      maxPixels = maxP)\n","\n","    # Initialize the export.\n","    batch.Task.start(export)\n","\n","    j+=1\n","\n","  return\n","\n","for elem in range(len(images)):\n","    export_image(ee.Image(images[elem]), image_names[elem], output_folders[elem])"]},{"cell_type":"markdown","metadata":{"id":"aHN8Alq_Ffmd"},"source":["# Export an RGB and each PC as its own raster"]},{"cell_type":"markdown","metadata":{"id":"SU6bdvSDp9X5"},"source":["Generate raster for RGB image"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5dPikEaPc7WU"},"outputs":[],"source":["# Add code here to save the RBG three band composite to disk\n","def rgb(im, out_name):\n","  im = ee.Image(im)\n","  rgb_minMax = minAndMax(im.select('B3'),roi)\n","\n","  rgb_viz = (im.select(['B4', 'B3', 'B2']).visualize(\n","             min = rgb_minMax.get('p5'),\n","             max = rgb_minMax.get('p95')))\n","\n","  rgb_out = batch.Export.image.toDrive(\n","    image = rgb_viz,\n","    region = roi,\n","    folder = 's2_outputs',\n","    description = f'6VPCA_{out_name}_rgb',\n","    scale = native_res,\n","    crs = 'EPSG:4326',\n","    maxPixels = maxP)\n","\n","  batch.Task.start(rgb_out)\n","\n","for elem in range(len(images)):\n","  rgb(im_list[elem],image_names[elem])"]},{"cell_type":"markdown","metadata":{"id":"DOco29kWbzoX"},"source":["# Extract raw files for thresholding"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2Am-zywVb4dd"},"outputs":[],"source":["# def export_image_raw(image, out_name, out_fold):\n","\n","#   exportViz_pc1 = image.select('PC_1')\n","#   exportViz_pc2 = image.select('PC_2')\n","#   exportViz_pc3 = image.select('PC_3')\n","#   exportViz_pc4 = image.select('PC_4')\n","#   exportViz_pc5 = image.select('PC_5')\n","#   exportViz_pc6 = image.select('PC_6')\n","\n","#   exp_ls = [\n","#             exportViz_pc1,\n","#             exportViz_pc2,\n","#             exportViz_pc3,\n","#             exportViz_pc4,\n","#             exportViz_pc5,\n","#             exportViz_pc6\n","#             ]\n","\n","#   j = 1\n","#   for im in exp_ls:\n","#     export = batch.Export.image.toDrive(\n","#       image = im,\n","#       region = roi,\n","#       folder = 's2_outputs_raw',\n","#       description = f'6VPCA_{out_name}_PC{j}_rawFile',\n","#       scale = native_res,\n","#       crs = 'EPSG:4326',\n","#       maxPixels = maxP)\n","\n","#     # Initialize the export.\n","#     batch.Task.start(export)\n","\n","#     j+=1\n","\n","#   return\n","\n","# for elem in range(len(images)):\n","#   export_image_raw(ee.Image(images[elem]), image_names[elem], output_folders[elem])"]},{"cell_type":"markdown","metadata":{"id":"pvaEokUiFepn"},"source":["## Code to calculate the weighted sum of the 3 leading components\n","\n"]},{"cell_type":"code","source":["def weighted_pcs3(im, out_name):\n","  pc_sum3 = im.expression(\n","      '(exVar_3*pc_3)', {\n","      'exVar_3' : ee.Array(forSumImages[1]).get([0]),\n","      'pc_3': ee.Image(forSumImages[0]).select('PC_3'),\n","  })\n","\n","  # now generate an output map\n","\n","  pc_sum3_minMax = minAndMax(pc_sum3,roi)\n","\n","  pc_sum3_viz = (pc_sum3.visualize(palette = spectral,\n","             min = pc_sum3_minMax.get('p1'),\n","             max = pc_sum3_minMax.get('p99')))\n","\n","  vpc_sum3 = batch.Export.image.toDrive(\n","    image = pc_sum3_viz,\n","    region = roi,\n","    description = f'6VPCA_{out_name}_C3Viz',\n","    folder = 'Component_Viz',\n","    scale = 10,\n","    crs = 'EPSG:4326',\n","    maxPixels = maxP)\n","\n","  batch.Task.start(vpc_sum3)\n","\n","for elem in range(len(images)):\n","  weighted_pcs3(ee.Image(images[elem]),image_names[elem])"],"metadata":{"id":"axat7x3a-OMy"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZNLptwmtFagX"},"outputs":[],"source":["# Now calculate the sum of the three VPC images to get a thematic image with noise removed\n","# Each principal component is rescaled by its extracted variance\n","def weighted_pcs1to3(im, out_name):\n","  pc_sum3 = im.expression(\n","      '(exVar_1*pc_1 + exVar_2*pc_2 + exVar_3*pc_3)', {\n","      'exVar_1' : ee.Array(forSumImages[1]).get([0]),\n","      'exVar_2' : ee.Array(forSumImages[1]).get([1]),\n","      'exVar_3' : ee.Array(forSumImages[1]).get([2]),\n","      'pc_1': ee.Image(forSumImages[0]).select('PC_1'),\n","      'pc_2': ee.Image(forSumImages[0]).select('PC_2'),\n","      'pc_3': ee.Image(forSumImages[0]).select('PC_3')\n","  })\n","\n","  # now generate an output map\n","\n","  pc_sum3_minMax = minAndMax(pc_sum3,roi)\n","\n","  pc_sum3_viz = (pc_sum3.visualize(palette = spectral,\n","             min = pc_sum3_minMax.get('p2'),\n","             max = pc_sum3_minMax.get('p98')))\n","\n","  vpc_sum3 = batch.Export.image.toDrive(\n","    image = pc_sum3_viz,\n","    region = roi,\n","    description = f'6VPCA_{out_name}_sum3',\n","    scale = 10,\n","    crs = 'EPSG:4326',\n","    maxPixels = maxP)\n","\n","  batch.Task.start(vpc_sum3)\n","\n","for elem in range(len(images)):\n","  weighted_pcs1to3(ee.Image(images[elem]),image_names[elem])"]},{"cell_type":"markdown","metadata":{"id":"kz61Qo6tN6qP"},"source":["## Code to calculate the weighted sum of the 6 extracted components\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wpDeKxqpN7Qx"},"outputs":[],"source":["# Now calculate the sum of the six extracted VPC images to get a thematic image with noise removed\n","# Each principal component is rescaled by its extracted variance\n","def weighted_pcs1to6(im, out_name):\n","  pc_sum6 = finalImage.expression(\n","      '(exVar_1*pc_1 + exVar_2*pc_2 + exVar_3*pc_3 + exVar_4*pc_4 + exVar_5*pc_5 + exVar_6*pc_6)', {\n","      'exVar_1' : ex_frac_var_flat.get([0]),\n","      'exVar_2' : ex_frac_var_flat.get([1]),\n","      'exVar_3' : ex_frac_var_flat.get([2]),\n","      'exVar_4' : ex_frac_var_flat.get([3]),\n","      'exVar_5' : ex_frac_var_flat.get([4]),\n","      'exVar_6' : ex_frac_var_flat.get([5]),\n","      'pc_1': finalImage.select('PC_1'),\n","      'pc_2': finalImage.select('PC_2'),\n","      'pc_3': finalImage.select('PC_3'),\n","      'pc_4': finalImage.select('PC_4'),\n","      'pc_5': finalImage.select('PC_5'),\n","      'pc_6': finalImage.select('PC_6')\n","  })\n","\n","  # now generate an output map\n","  pc_sum6_minMax = minAndMax(pc_sum6,roi)\n","\n","  pc_sum6_viz = (pc_sum6.visualize(palette = spectral,\n","             min = pc_sum6_minMax.get('p2'),\n","             max = pc_sum6_minMax.get('p98')))\n","\n","  vpc_sum6 = batch.Export.image.toDrive(\n","    image = pc_sum6_viz,\n","    region = water_geom.geometry().bounds().getInfo()['coordinates'],\n","    folder = 'KSU_VPCA_Outputs',\n","    description = f'6VPCA_{out_name}_sum6',\n","    scale = 30,\n","    crs = 'EPSG:4326',\n","    maxPixels = 1e8)\n","  # Comment out line below to prevent write of file to disk\n","  batch.Task.start(vpc_sum6)\n","\n","for elem in range(len(images)):\n","  weighted_pcs1to3(ee.Image(images[elem]),image_names[elem])"]}],"metadata":{"colab":{"collapsed_sections":["kz61Qo6tN6qP"],"provenance":[{"file_id":"1JS_hTZen1667bb8YqBM_MpXlnqJNv_IE","timestamp":1709767042056},{"file_id":"1xmjmN7idYCUmb6ZYWIyAyN77oeX3sytF","timestamp":1647035737289},{"file_id":"1R85pYzD221xucLU0pRFXCup4EsN5lwR9","timestamp":1622663530508},{"file_id":"1CfktWAilbA-QQ9nYBxBupb8bWAWAeUF_","timestamp":1622578425957},{"file_id":"1uQEFF5XgtkMkFf8iTk6WA0eBXbE64xft","timestamp":1621000243336},{"file_id":"1nknOhKoR45hgKebKA6oasiC-ZZVDdxjq","timestamp":1620920966369},{"file_id":"1fZBpivL_zApp1r4GvD3O5KyclJ732k6C","timestamp":1602369822067},{"file_id":"16dXaa8q1TvX7IpLfQmErzIxodkyWkOcM","timestamp":1600521505550},{"file_id":"1VRrwnVinUgV8KX776Ks1W2LbtRlkAVxQ","timestamp":1600289136541}]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}